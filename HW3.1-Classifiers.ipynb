{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d326d7d",
   "metadata": {},
   "source": [
    "This can be run [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework_3/blob/main/HW3.1-Classifiers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd40e9d",
   "metadata": {},
   "source": [
    "## MNIST Classifiers (Convolutional Neural Networks and Fully Connected Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20290e",
   "metadata": {},
   "source": [
    "<b>Optional</b>: Installing Wandb to see cool analysis of you code. You can go through the documentation here. We will do it for this assignment to get a taste of the GPU and CPU utilizations. If this is creating problems to your code, please comment out all the wandb lines from the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f63cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below line to install wandb (optinal)\n",
    "# !pip install wandb \n",
    "# Uncomment the below line to install torchinfo (https://github.com/TylerYep/torchinfo) [Mandatory]\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8211a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-10-20 13:57:20--  https://cs7150.baulab.info/2022-Fall/data/mnist-classify.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving cs7150.baulab.info (cs7150.baulab.info)... 35.232.255.106\n",
      "Connecting to cs7150.baulab.info (cs7150.baulab.info)|35.232.255.106|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘mnist-classify.pth’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -N https://cs7150.baulab.info/2022-Fall/data/mnist-classify.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621c2c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split,Subset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05fd6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py\", line 1166, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py\", line 308, in setup\n",
      "    wandb_login._login(\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py\", line 298, in _login\n",
      "    wlogin.prompt_api_key()\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py\", line 221, in prompt_api_key\n",
      "    key, status = self._prompt_api_key()\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py\", line 201, in _prompt_api_key\n",
      "    key = apikey.prompt_api_key(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/wandb/sdk/lib/apikey.py\", line 144, in prompt_api_key\n",
      "    key = input_callback(api_ask).strip()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/click/termui.py\", line 164, in prompt\n",
      "    value = prompt_func(prompt)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/karan_mudaliar/miniconda3/lib/python3.11/site-packages/click/termui.py\", line 147, in prompt_func\n",
      "    raise Abort() from None\n",
      "click.exceptions.Abort\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "An unexpected error occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1166\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1165\u001b[0m wi \u001b[39m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1166\u001b[0m wi\u001b[39m.\u001b[39;49msetup(kwargs)\n\u001b[1;32m   1167\u001b[0m \u001b[39massert\u001b[39;00m wi\u001b[39m.\u001b[39msettings\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:308\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_offline \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_noop:\n\u001b[0;32m--> 308\u001b[0m     wandb_login\u001b[39m.\u001b[39;49m_login(\n\u001b[1;32m    309\u001b[0m         anonymous\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39manonymous\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    310\u001b[0m         force\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mforce\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    311\u001b[0m         _disable_warning\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    312\u001b[0m         _silent\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mquiet \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49msilent,\n\u001b[1;32m    313\u001b[0m         _entity\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mentity\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49mentity,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    316\u001b[0m \u001b[39m# apply updated global state after login was handled\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:298\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m key:\n\u001b[0;32m--> 298\u001b[0m     wlogin\u001b[39m.\u001b[39;49mprompt_api_key()\n\u001b[1;32m    300\u001b[0m \u001b[39m# make sure login credentials get to the backend\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:221\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt_api_key\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     key, status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt_api_key()\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m==\u001b[39m ApiKeyStatus\u001b[39m.\u001b[39mNOTTY:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:201\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     key \u001b[39m=\u001b[39m apikey\u001b[39m.\u001b[39;49mprompt_api_key(\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings,\n\u001b[1;32m    203\u001b[0m         api\u001b[39m=\u001b[39;49mapi,\n\u001b[1;32m    204\u001b[0m         no_offline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    205\u001b[0m         no_create\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m     \u001b[39m# invalid key provided, try again\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/wandb/sdk/lib/apikey.py:144\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    141\u001b[0m     wandb\u001b[39m.\u001b[39mtermlog(\n\u001b[1;32m    142\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou can find your API key in your browser here: \u001b[39m\u001b[39m{\u001b[39;00mapp_url\u001b[39m}\u001b[39;00m\u001b[39m/authorize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n\u001b[0;32m--> 144\u001b[0m     key \u001b[39m=\u001b[39m input_callback(api_ask)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    145\u001b[0m write_key(settings, key, api\u001b[39m=\u001b[39mapi)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/click/termui.py:164\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     value \u001b[39m=\u001b[39m prompt_func(prompt)\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m value:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/click/termui.py:147\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    146\u001b[0m     echo(\u001b[39mNone\u001b[39;00m, err\u001b[39m=\u001b[39merr)\n\u001b[0;32m--> 147\u001b[0m \u001b[39mraise\u001b[39;00m Abort() \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAbort\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/karan_mudaliar/Documents/GitHub/CS7150-Homework_3/HW3.1-Classifiers.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karan_mudaliar/Documents/GitHub/CS7150-Homework_3/HW3.1-Classifiers.ipynb#Y135sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create an account at https://wandb.ai/site and paste the api key here (optional)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karan_mudaliar/Documents/GitHub/CS7150-Homework_3/HW3.1-Classifiers.ipynb#Y135sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karan_mudaliar/Documents/GitHub/CS7150-Homework_3/HW3.1-Classifiers.ipynb#Y135sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhw3.1-ConvNets\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1208\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             wandb\u001b[39m.\u001b[39mtermerror(\u001b[39m\"\u001b[39m\u001b[39mAbnormal program exit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1207\u001b[0m             os\u001b[39m.\u001b[39m_exit(\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1208\u001b[0m         \u001b[39mraise\u001b[39;00m Error(\u001b[39m\"\u001b[39m\u001b[39mAn unexpected error occurred\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror_seen\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "\u001b[0;31mError\u001b[0m: An unexpected error occurred"
     ]
    }
   ],
   "source": [
    "# Create an account at https://wandb.ai/site and paste the api key here (optional)\n",
    "import wandb\n",
    "wandb.init(project=\"hw3.1-ConvNets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab5423",
   "metadata": {},
   "source": [
    "### Some helper functions to view network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_network_parameters(model):\n",
    "    # Visualise the number of parameters\n",
    "    tensor_list = list(model.state_dict().items())\n",
    "    total_parameters = 0\n",
    "    print('Model Summary\\n')\n",
    "    for layer_tensor_name, tensor in tensor_list:\n",
    "        total_parameters += int(torch.numel(tensor))\n",
    "        print('{}: {} elements'.format(layer_tensor_name, torch.numel(tensor)))\n",
    "    print(f'\\nTotal Trainable Parameters: {total_parameters}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a073a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_network_shapes(model, input_shape):\n",
    "    print(summary(conv_net, input_size=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140bdae",
   "metadata": {},
   "source": [
    "### Fully Connected Network for Image Classification\n",
    "Let's build a simple fully connected network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf368fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fc_net():\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(1*28*28,8*28*28),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8*28*28,16*14*14),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*14*14,32*7*7),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32*7*7,288),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(288,64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,10),\n",
    "        nn.LogSoftmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_net = simple_fc_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_network_parameters(fc_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(fc_net, input_size=(1, 1, 28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfc411",
   "metadata": {},
   "source": [
    "<b>Exercise</b>: Now try to add different layers and see how the network parameters vary. Does adding layers reduce the parameters? Does the number of hidden neurons in the layers affect the total trainable parameters? \n",
    "\n",
    "<i>Add a few sentences on your observations while using various architectures</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please type your answer here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bc885",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network for Image Classification\n",
    "Let's build a simple CNN to classify our images. \n",
    "<b> Exercise 3.1.1:</b> In the function below please add the conv/Relu/Maxpool layers to match the shape of FC-Net. Suppose at the some layer the FC-Net has `28*28*16` dimension, we want your conv_net to have `16 X 28 X 28` shape at the same numbered layer. <br>\n",
    "<b>Extra-credit:</b> Try not to use MaxPool2d !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65674742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_conv_net():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1,16,kernel_size=3,padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        # TO-DO: Add layers below\n",
    "        '''\n",
    "        Add your code here to match the output shape of the FC-Net \n",
    "        '''\n",
    "        # TO-DO, what will your shape be after you flatten? Fill it in place of None\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(None,64),\n",
    "        # Do not change the code below\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,10),\n",
    "        nn.LogSoftmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb06a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = simple_conv_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6e8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_network_parameters(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_network_shapes(conv_net, input_shape=(1,1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f577403",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1.2</b>: Why is the final layer a log softmax? What is a softmax function? Can we use ReLU instead of softmax? If yes, what would you do different? If not, tell us why. If you think there is a different answer, feel free to use this space to chart it down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530f3e2",
   "metadata": {},
   "source": [
    " Please type your answer here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea2c8c",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1.3</b>: What is the ratio of number of parameters of Conv-net to number of parameters of FC-Net <br>\n",
    "$\\frac{p_{conv-net}}{p_{fc-net}}$ = Fill your answer <br>\n",
    "Do you see the difference ?! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6329e",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1.4</b>: Now try to add different layers and see how the network parameters vary. Does adding layers reduce the parameters? Does the number of hidden neurons in the layers affect the total trainable parameters? Use the `build_custom_fc_net` function given below. You do not have to understand the working of it. \n",
    "\n",
    "<i>Add a few sentences on your observations while using various architectures</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d76f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_custom_fc_net(inp_dim: int, out_dim: int, hidden_fc_dim: List[int]):\n",
    "    '''\n",
    "    Inputs :\n",
    "    \n",
    "    inp_dim: Shape of the input dimensions (in MNIST case 28*28)\n",
    "    out_dim: Desired classification classes (in MNIST case 10)\n",
    "    hidden_fc_dim: List of the intermediate dimension shapes (list of integers). Try different values and see the shapes'\n",
    "    \n",
    "    Return: nn.Sequential (final custom model)\n",
    "    '''\n",
    "    assert type(hidden_fc_dim) == list, \"Please define hidden_fc_dim as list of integers\"\n",
    "    layers = []\n",
    "    layers.append((f'flatten', nn.Flatten()))\n",
    "    # If no hidden layer is required\n",
    "    if len(hidden_fc_dim) == 0:\n",
    "        layers.append((f'linear',nn.Linear(math.prod(inp_dim),out_dim)))\n",
    "        layers.append((f'activation',nn.LogSoftmax()))\n",
    "    else:\n",
    "        # Loop over hidden dimensions and add layers\n",
    "        for idx, dim in enumerate(hidden_fc_dim):\n",
    "            if idx == 0:\n",
    "                layers.append((f'linear_{idx+1}',nn.Linear(math.prod(inp_dim),dim)))\n",
    "                layers.append((f'activation_{idx+1}',nn.ReLU()))\n",
    "            else:\n",
    "                layers.append((f'linear_{idx+1}',nn.Linear(hidden_fc_dim[idx-1],dim)))\n",
    "                layers.append((f'activation_{idx+1}',nn.ReLU()))\n",
    "        layers.append((f'linear_{idx+2}',nn.Linear(dim,out_dim)))\n",
    "        layers.append((f'activation_{idx+2}',nn.LogSoftmax()))\n",
    "        \n",
    "    model =  nn.Sequential(OrderedDict(layers))\n",
    "    return model\n",
    "\n",
    "# TO-DO build different networks (atleast 3) and see the parameters\n",
    "#(You don't have to understand the function above. It is a generic way to build a FC-Net)\n",
    "\n",
    "\n",
    "fc_net_custom1 = build_custom_fc_net(inp_dim=(1,28,28), out_dim=10, hidden_fc_dim=[128,64,32])\n",
    "view_network_parameters(fc_net_custom1)\n",
    "\n",
    "# fc_net_custom2 = \n",
    "# view_network_parameters(fc_net_custom2)\n",
    "\n",
    "# fc_net_custom3 = \n",
    "# view_network_parameters(fc_net_custom3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c48647",
   "metadata": {},
   "source": [
    "## Let's train the models to see their performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading mnist into folder\n",
    "data_dir = 'data' # make sure that this folder is created in your working dir\n",
    "# transform the PIL images to tensor using torchvision.transform.toTensor method\n",
    "train_data = torchvision.datasets.MNIST(data_dir, train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
    "test_data  = torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
    "print(f'Datatype of the dataset object: {type(train_data)}')\n",
    "# check the length of dataset\n",
    "n_train_samples = len(train_data)\n",
    "print(f'Number of samples in training data: {len(train_data)}')\n",
    "print(f'Number of samples in test data: {len(test_data)}')\n",
    "# Check the format of dataset\n",
    "#print(f'Foramt of the dataset: \\n {train_data}')\n",
    "\n",
    "val_split = .2\n",
    "batch_size=256 \n",
    "\n",
    "train_data_, val_data = random_split(train_data, [int(n_train_samples*(1-val_split)), int(n_train_samples*val_split)])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data_, batch_size=batch_size,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94c083",
   "metadata": {},
   "source": [
    "### Displaying the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e15476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2, 3, i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(train_data[i][0][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Class Label: {}\".format(train_data[i][1]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524851bc",
   "metadata": {},
   "source": [
    "## Function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, device, loss_fn, optimizer, input_dim=(-1,1,28,28)):\n",
    "    model.train()\n",
    "    # Initiate a loss monitor\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning and not supervised classification)\n",
    "    for images, labels in train_loader: # the variable `labels` will be used for customised training\n",
    "        # reshape input\n",
    "        images = torch.reshape(images,input_dim)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # predict the class\n",
    "        predicted = model(images)\n",
    "        loss = loss_fn(predicted, labels)\n",
    "        # Backward pass (back propagation)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        wandb.log({\"Training Loss\": loss})\n",
    "        wandb.watch(model)\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d74a4",
   "metadata": {},
   "source": [
    "## Function to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Function\n",
    "def test_model(model, test_loader, device, loss_fn, input_dim=(-1,1,28,28)):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        predicted = []\n",
    "        actual = []\n",
    "        for images, labels in test_loader:\n",
    "            # reshape input\n",
    "            images = torch.reshape(images,input_dim)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ## predict the label\n",
    "            pred = model(images)\n",
    "            # Append the network output and the original image to the lists\n",
    "            predicted.append(pred.cpu())\n",
    "            actual.append(labels.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        predicted = torch.cat(predicted)\n",
    "        actual = torch.cat(actual) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(predicted, actual)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd1864",
   "metadata": {},
   "source": [
    "Before we start training let's delete the huge FC-Net we built and build a reasonable FC-Net (You learnt why such larger networks are not reasonable in the previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fca7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del fc_net, fc_net_custom1, fc_net_custom2, fc_net_custom3\n",
    "torch.cuda.empty_cache()\n",
    "# Building a reasonable fully connected network\n",
    "fc_net = build_custom_fc_net(inp_dim=(1,28,28), out_dim=10, hidden_fc_dim=[128,64,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc501cf",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1.5:</b> \n",
    "Code the `weight_init_xavier` function by referring to https://pytorch.org/docs/stable/nn.init.html. Replace the weight initializations to your own function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60636ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "# Choosing a device based on the env and torch setup\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "def weight_init_zero(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.constant_(m.weight, 0.0)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_init_xavier(m):\n",
    "    '''\n",
    "    TO-DO: please add code below to add xavier uniform initialization and remove the 'pass'\n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "    \n",
    "fc_net.to(device)\n",
    "conv_net.to(device)\n",
    "\n",
    "# Apply the weight initialization\n",
    "fc_net.apply(weight_init_zero)\n",
    "conv_net.apply(weight_init_zero)\n",
    "\n",
    "# Apply the xavier weight initialization\n",
    "#TO-DO: Add your function here\n",
    "fc_net.apply(weight_init_xavier)\n",
    "conv_net.apply(weight_init_xavier)\n",
    "\n",
    "\n",
    "# Take the parameters for optimiser\n",
    "params_to_optimize_fc = [\n",
    "    {'params': fc_net.parameters()}\n",
    "]\n",
    "\n",
    "params_to_optimize_conv = [\n",
    "    {'params': conv_net.parameters()}\n",
    "]\n",
    "### Define the loss function\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr= 0.001\n",
    "\n",
    "optim_fc = torch.optim.Adam(params_to_optimize_fc, lr=lr, weight_decay=1e-05)\n",
    "optim_conv = torch.optim.Adam(params_to_optimize_conv, lr=lr, weight_decay=1e-05)\n",
    "num_epochs = 30\n",
    "wandb.config = {\n",
    "  \"learning_rate\": lr,\n",
    "  \"epochs\": num_epochs,\n",
    "  \"batch_size\": batch_size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53753ff1",
   "metadata": {},
   "source": [
    "# Training the Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9616b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Conv Net training started')\n",
    "history_conv = {'train_loss':[],'val_loss':[]}\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ### Training \n",
    "    \n",
    "    train_loss = train_model(\n",
    "        model=conv_net,\n",
    "        train_loader=train_loader,\n",
    "        device=device,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optim_conv,\n",
    "        input_dim=(-1,1,28,28))\n",
    "    ### Validation  (use the testing function)\n",
    "    val_loss = test_model(\n",
    "        model=conv_net,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        loss_fn=loss_fn,\n",
    "        input_dim=(-1,1,28,28))\n",
    "    # Print Losses \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} : train loss {train_loss:.3f} \\t val loss {val_loss:.3f}')\n",
    "    history_conv['train_loss'].append(train_loss)\n",
    "    history_conv['val_loss'].append(val_loss)\n",
    "    \n",
    "    \n",
    "print(f'Conv Net training done in {(datetime.datetime.now()-start_time).total_seconds():.3f} seconds!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1465fbd",
   "metadata": {},
   "source": [
    "### Visualizing Training Progress of Conv Net (Also check out your wandb.ai homepage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1622763",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history_conv['train_loss'], color='blue')\n",
    "plt.plot(history_conv['val_loss'], color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Negative Log Likelihood Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163e64a",
   "metadata": {},
   "source": [
    "### Visualizing Predictions of Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac3520",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "with torch.no_grad():\n",
    "    example_data = example_data.to(device)\n",
    "    output = conv_net(example_data)\n",
    "example_data = example_data.cpu().detach().numpy()\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray',interpolation='none')\n",
    "    plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd45fe",
   "metadata": {},
   "source": [
    "# Training the Fully-Connected Neural Networks\n",
    "\n",
    "<b>Exercise 3.1.6:</b> Train the fully connected neural network and analyse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO:Train the fc_net here\n",
    "print('FC Net training started')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5cd18",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress of FC Net (Check out your wandb.ai project webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Visualize the training progress of fc_net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c77be",
   "metadata": {},
   "source": [
    "## Visualizing Predictions of FC Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f399e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Visualise the predictions of fc_net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2981154",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1.7</b>: What are the training times for each of the model? Did both the models take similar times? If yes, why? Shouldn't CNN train faster given it's number of weights to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please type your answer here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33605f1",
   "metadata": {},
   "source": [
    "## Let's see how the models perform under translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d24b6",
   "metadata": {},
   "source": [
    "In principle, one of the advantages of convolutions is that they are equivariant under translation which means that a function composed out of convolutions should invariant under translation.\n",
    "\n",
    "<b>Exercise 3.1.8</b>: In practice, however, we might not see perfect invariance under translation.  What aspect of our network leads to imperfect invariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cbdfc",
   "metadata": {},
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0d35b",
   "metadata": {},
   "source": [
    "We will next measure the sensitivity  of the convolutional network to translation in practice, and we will compare it to the fully-connected version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8583d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to check accuracies for unit translation\n",
    "def shiftVsAccuracy(model, test_loader, device, loss_fn, shifts = 12, input_dim=(-1,1,28,28)):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    accuracies = []\n",
    "    shifted = []\n",
    "    for i in range(-shifts,shifts):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # No need to track the gradients\n",
    "            # Define the lists to store the outputs for each batch\n",
    "            predicted = []\n",
    "            actual = []\n",
    "            for images, labels in test_loader:\n",
    "                # reshape input\n",
    "                images = torch.roll(images,shifts=i, dims=2)\n",
    "                if i == 0:\n",
    "                    pass\n",
    "                elif i > 0:\n",
    "                    images[:,:,:i,:] = 0\n",
    "                else:\n",
    "                    images[:,:,i:,:] = 0\n",
    "                images = torch.reshape(images,input_dim)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                ## predict the label\n",
    "                pred = model(images)\n",
    "                # Append the network output and the original image to the lists\n",
    "                _ , pred = torch.max(pred.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                predicted.append(pred.cpu())\n",
    "                actual.append(labels.cpu())  \n",
    "            shifted.append(images[0][0].cpu())\n",
    "            acc = 100 * correct // total\n",
    "            accuracies.append(acc)\n",
    "    return accuracies,shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89576a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies,shifted = shiftVsAccuracy(\n",
    "        model=conv_net,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        shifts=12,\n",
    "        loss_fn=loss_fn,\n",
    "        input_dim=(-1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = np.arange(-12,12)\n",
    "plt.plot(shifts,accuracies)\n",
    "plt.title('Accuracy Vs Translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2320b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt_num = 0\n",
    "for i in range(-12,12):\n",
    "    plt.subplot(5,6,plt_num+1)\n",
    "    plt.imshow(shifted[plt_num], cmap='gray',interpolation='none')\n",
    "    plt.title(f\"Shifted: {i} Accuracy: {accuracies[plt_num]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b217e71",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1.8:</b>\n",
    "Do the same for FC-Net and plot the accuracies. Is the rate of accuracy degradation same as Conv-Net? Can you justify why this happened? <br>\n",
    "Clue: You might want to look at the way convolution layers process information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-DO Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d57f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
